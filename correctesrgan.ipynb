{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UQZzxLabCIl-6rFP5wTcMIrC3XV7-rfL",
      "authorship_tag": "ABX9TyMeCXQH/e8gJMptGUsA+lxW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnshq/ESRGAN/blob/main/correctesrgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlL0rDlEgWZb",
        "outputId": "fbd74dc7-bca6-459b-ff50-cc7fd3d0eec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted and Kaggle API configured!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup Kaggle API\n",
        "!mkdir -p ~/.kaggle/\n",
        "!cp \"/content/drive/MyDrive/kaggle/kaggle.json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!python -m pip install -qq kaggle\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted and Kaggle API configured!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYLviGGajIZB",
        "outputId": "e335b07b-976f-4bfd-b9a2-4aba57ec90f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download DIV2K dataset from Kaggle\n",
        "!kaggle datasets download -d joe1995/div2k-dataset\n",
        "\n",
        "# Extract the dataset\n",
        "!unzip -qq \"div2k-dataset.zip\"\n",
        "\n",
        "# Check the extracted structure\n",
        "import os\n",
        "print(\"üìÅ Dataset structure:\")\n",
        "for dirname, _, filenames in os.walk('/content'):\n",
        "    if 'DIV2K' in dirname or 'div2k' in dirname:\n",
        "        print(f\"üìÇ {dirname}\")\n",
        "        if filenames:\n",
        "            print(f\"   Files: {len(filenames)} items\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1BEzoypiTWV",
        "outputId": "c559471a-b5dd-4f12-9e30-856be270134d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/joe1995/div2k-dataset\n",
            "License(s): unknown\n",
            "Downloading div2k-dataset.zip to /content\n",
            " 99% 3.66G/3.71G [00:39<00:01, 33.2MB/s]\n",
            "100% 3.71G/3.71G [00:39<00:00, 101MB/s] \n",
            "üìÅ Dataset structure:\n",
            "üìÇ /content/DIV2K_train_HR\n",
            "üìÇ /content/DIV2K_train_HR/DIV2K_train_HR\n",
            "   Files: 800 items\n",
            "üìÇ /content/DIV2K_valid_HR\n",
            "üìÇ /content/DIV2K_valid_HR/DIV2K_valid_HR\n",
            "   Files: 100 items\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create organized dataset structure\n",
        "project_path = '/content/ESRGAN_Project'\n",
        "os.makedirs(f'{project_path}/dataset/train/hr', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/dataset/train/lr', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/dataset/valid/hr', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/dataset/valid/lr', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/models', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/results', exist_ok=True)\n",
        "\n",
        "# Find DIV2K folders and organize\n",
        "div2k_path = None\n",
        "hr_source = None\n",
        "lr_source = None\n",
        "\n",
        "# Search for the actual image folders\n",
        "print(\"üîç Searching for DIV2K dataset...\")\n",
        "for root, dirs, files in os.walk('/content'):\n",
        "    # Look for folders containing actual images\n",
        "    for d in dirs:\n",
        "        if 'DIV2K_train_HR' in d and not div2k_path:\n",
        "            potential_hr = os.path.join(root, d)\n",
        "            # Check if this folder contains image files (not just subdirectories)\n",
        "            contents = os.listdir(potential_hr)\n",
        "            image_files = [f for f in contents if f.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.isfile(os.path.join(potential_hr, f))]\n",
        "\n",
        "            if image_files:\n",
        "                hr_source = potential_hr\n",
        "                print(f\"‚úÖ Found HR images folder: {hr_source}\")\n",
        "                break\n",
        "            else:\n",
        "                # Check if there's a nested folder with the same name\n",
        "                nested_hr = os.path.join(potential_hr, 'DIV2K_train_HR')\n",
        "                if os.path.exists(nested_hr):\n",
        "                    nested_contents = os.listdir(nested_hr)\n",
        "                    nested_images = [f for f in nested_contents if f.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.isfile(os.path.join(nested_hr, f))]\n",
        "                    if nested_images:\n",
        "                        hr_source = nested_hr\n",
        "                        print(f\"‚úÖ Found HR images in nested folder: {hr_source}\")\n",
        "                        break\n",
        "\n",
        "# Search for LR folder\n",
        "for root, dirs, files in os.walk('/content'):\n",
        "    for d in dirs:\n",
        "        if 'DIV2K_train_LR' in d:\n",
        "            potential_lr_root = os.path.join(root, d)\n",
        "            # Look for X4 subfolder\n",
        "            x4_folder = os.path.join(potential_lr_root, 'X4')\n",
        "            if os.path.exists(x4_folder):\n",
        "                lr_source = x4_folder\n",
        "                print(f\"‚úÖ Found LR images folder: {lr_source}\")\n",
        "                break\n",
        "            # Check for nested structure\n",
        "            elif os.path.exists(os.path.join(potential_lr_root, 'DIV2K_train_LR_bicubic', 'X4')):\n",
        "                lr_source = os.path.join(potential_lr_root, 'DIV2K_train_LR_bicubic', 'X4')\n",
        "                print(f\"‚úÖ Found LR images in nested folder: {lr_source}\")\n",
        "                break\n",
        "\n",
        "if hr_source and os.path.exists(hr_source):\n",
        "    # Get only image files, not directories\n",
        "    hr_items = os.listdir(hr_source)\n",
        "    hr_files = [f for f in hr_items if f.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.isfile(os.path.join(hr_source, f))]\n",
        "\n",
        "    print(f\"üì∏ Found {len(hr_files)} HR training images\")\n",
        "\n",
        "    if hr_files:\n",
        "        # Copy first 700 images for training, rest for validation\n",
        "        train_count = min(700, len(hr_files))\n",
        "\n",
        "        print(f\"üìÅ Copying {train_count} images for training...\")\n",
        "        for i, file in enumerate(hr_files[:train_count]):\n",
        "            src_path = os.path.join(hr_source, file)\n",
        "            dst_path = os.path.join(f'{project_path}/dataset/train/hr/', file)\n",
        "\n",
        "            # Double-check it's actually a file before copying\n",
        "            if os.path.isfile(src_path):\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Skipping {file} - not a file\")\n",
        "\n",
        "        # Copy remaining for validation\n",
        "        if len(hr_files) > train_count:\n",
        "            val_files = hr_files[train_count:]\n",
        "            print(f\"üìÅ Copying {len(val_files)} images for validation...\")\n",
        "            for file in val_files:\n",
        "                src_path = os.path.join(hr_source, file)\n",
        "                dst_path = os.path.join(f'{project_path}/dataset/valid/hr/', file)\n",
        "\n",
        "                if os.path.isfile(src_path):\n",
        "                    shutil.copy2(src_path, dst_path)\n",
        "\n",
        "    # Handle LR images if they exist\n",
        "    if lr_source and os.path.exists(lr_source):\n",
        "        lr_items = os.listdir(lr_source)\n",
        "        lr_files = [f for f in lr_items if f.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.isfile(os.path.join(lr_source, f))]\n",
        "\n",
        "        print(f\"üì∏ Found {len(lr_files)} LR training images\")\n",
        "\n",
        "        # Copy corresponding LR images\n",
        "        train_count = min(700, len(lr_files))\n",
        "        for file in lr_files[:train_count]:\n",
        "            src_path = os.path.join(lr_source, file)\n",
        "            dst_path = os.path.join(f'{project_path}/dataset/train/lr/', file)\n",
        "\n",
        "            if os.path.isfile(src_path):\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "\n",
        "        # Copy remaining for validation\n",
        "        if len(lr_files) > train_count:\n",
        "            for file in lr_files[train_count:]:\n",
        "                src_path = os.path.join(lr_source, file)\n",
        "                dst_path = os.path.join(f'{project_path}/dataset/valid/lr/', file)\n",
        "\n",
        "                if os.path.isfile(src_path):\n",
        "                    shutil.copy2(src_path, dst_path)\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No LR images found - will generate them from HR images during training\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå DIV2K dataset folders not found!\")\n",
        "\n",
        "print(\"\\nüìÅ Final dataset structure:\")\n",
        "print(f\"Training HR: {len(os.listdir(f'{project_path}/dataset/train/hr'))} images\")\n",
        "print(f\"Training LR: {len(os.listdir(f'{project_path}/dataset/train/lr'))} images\")\n",
        "print(f\"Validation HR: {len(os.listdir(f'{project_path}/dataset/valid/hr'))} images\")\n",
        "print(f\"Validation LR: {len(os.listdir(f'{project_path}/dataset/valid/lr'))} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjqkDZ24iWRU",
        "outputId": "2a1fd335-027b-48d7-f7ac-00cf37ba47d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching for DIV2K dataset...\n",
            "‚úÖ Found HR images in nested folder: /content/DIV2K_train_HR/DIV2K_train_HR\n",
            "‚úÖ Found HR images folder: /content/DIV2K_train_HR/DIV2K_train_HR\n",
            "üì∏ Found 800 HR training images\n",
            "üìÅ Copying 700 images for training...\n",
            "üìÅ Copying 100 images for validation...\n",
            "‚ÑπÔ∏è No LR images found - will generate them from HR images during training\n",
            "\n",
            "üìÅ Final dataset structure:\n",
            "Training HR: 700 images\n",
            "Training LR: 0 images\n",
            "Validation HR: 100 images\n",
            "Validation LR: 0 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q opencv-python pillow matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY_ltYTAiXrY",
        "outputId": "be08a9f5-b624-4287-9a20-b7cc7fcf4dad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ All libraries imported successfully!\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model save/load configuration\n",
        "MODEL_SAVE_PATH = f\"{project_path}/models/esrgan_generator.pth\"\n",
        "CHECKPOINT_PATH = f\"{project_path}/models/esrgan_checkpoint.pth\"\n",
        "\n",
        "def save_model(generator, discriminator, optimizer_gen, optimizer_disc, epoch, loss):\n",
        "    \"\"\"Save model weights and training state\"\"\"\n",
        "    checkpoint = {\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
        "        'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': loss\n",
        "    }\n",
        "\n",
        "    # Save complete checkpoint\n",
        "    torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "\n",
        "    # Save just generator for inference\n",
        "    torch.save(generator.state_dict(), MODEL_SAVE_PATH)\n",
        "\n",
        "    print(f\"üíæ Model saved! Generator: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "def load_model_for_inference(generator, device):\n",
        "    \"\"\"Load generator weights for inference only\"\"\"\n",
        "    if os.path.exists(MODEL_SAVE_PATH):\n",
        "        generator.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
        "        generator.eval()\n",
        "        print(f\"‚úÖ Generator loaded from {MODEL_SAVE_PATH}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå No saved model found at {MODEL_SAVE_PATH}\")\n",
        "        return False\n",
        "\n",
        "def load_checkpoint(generator, discriminator, optimizer_gen, optimizer_disc, device):\n",
        "    \"\"\"Load complete training checkpoint to resume training\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_PATH):\n",
        "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
        "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"‚úÖ Checkpoint loaded! Resuming from epoch {start_epoch}\")\n",
        "        return start_epoch\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No checkpoint found, starting fresh training\")\n",
        "        return 0\n",
        "\n",
        "def check_trained_model_exists():\n",
        "    \"\"\"Check if a trained model already exists\"\"\"\n",
        "    return os.path.exists(MODEL_SAVE_PATH)\n",
        "\n",
        "print(\"‚úÖ Save/Load functions defined!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LgOz81IieBH",
        "outputId": "90d85dad-b936-4d4a-f037-176bda8b00b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Save/Load functions defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, num_feat=64, num_grow_ch=32):\n",
        "        super(ResidualDenseBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_feat, num_grow_ch, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(num_feat + num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(num_feat + 2 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv4 = nn.Conv2d(num_feat + 3 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv5 = nn.Conv2d(num_feat + 4 * num_grow_ch, num_feat, 3, 1, 1)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lrelu(self.conv1(x))\n",
        "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
        "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
        "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        return x5 * 0.2 + x\n",
        "\n",
        "print(\"‚úÖ ResidualDenseBlock defined!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6vPjvcMifE3",
        "outputId": "88276545-a483-4647-d1a9-7bc0e0868c93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ResidualDenseBlock defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RRDB(nn.Module):\n",
        "    def __init__(self, num_feat, num_grow_ch=32):\n",
        "        super(RRDB, self).__init__()\n",
        "        self.rdb1 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "        self.rdb2 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "        self.rdb3 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.rdb1(x)\n",
        "        out = self.rdb2(out)\n",
        "        out = self.rdb3(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "print(\"‚úÖ RRDB Block defined!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvLsuG3eijPO",
        "outputId": "c4d62530-e2e5-4fa9-e4e2-023b0465999c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RRDB Block defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_channels=64, num_blocks=23):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # RRDB blocks\n",
        "        self.trunk = nn.Sequential(*[RRDB(num_channels) for _ in range(num_blocks)])\n",
        "\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Upsampling layers\n",
        "        self.upconv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.upconv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Final output layer\n",
        "        self.conv3 = nn.Conv2d(num_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fea = self.conv1(x)\n",
        "        trunk = self.trunk(fea)\n",
        "        fea = self.conv2(trunk) + fea\n",
        "\n",
        "        # Upsampling\n",
        "        fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
        "        fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
        "\n",
        "        out = self.conv3(fea)\n",
        "        return torch.tanh(out)\n",
        "\n",
        "print(\"‚úÖ Generator model defined!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lquEZCSfil7I",
        "outputId": "a19b0fbe-3b84-4a3e-8ec8-72a49cc03978"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generator model defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, 2, 1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels, 64, normalize=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.Conv2d(512, 1, 4, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "print(\"‚úÖ Discriminator model defined!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBt95Zfainpj",
        "outputId": "e7b24f7b-f6eb-42f3-d091-368fc5075671"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Discriminator model defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DIV2KDataset(Dataset):\n",
        "    def __init__(self, hr_folder, lr_folder, transform=None, crop_size=128):\n",
        "        self.hr_folder = hr_folder\n",
        "        self.lr_folder = lr_folder\n",
        "        self.transform = transform\n",
        "        self.crop_size = crop_size\n",
        "\n",
        "        # Get all image files\n",
        "        self.hr_images = [f for f in os.listdir(hr_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        print(f\"üì∏ Found {len(self.hr_images)} training images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hr_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hr_filename = self.hr_images[idx]\n",
        "\n",
        "        # Create corresponding LR filename\n",
        "        lr_filename = hr_filename.replace('.png', 'x4.png')\n",
        "\n",
        "        hr_path = os.path.join(self.hr_folder, hr_filename)\n",
        "        lr_path = os.path.join(self.lr_folder, lr_filename)\n",
        "\n",
        "        # Load images\n",
        "        hr_image = Image.open(hr_path).convert('RGB')\n",
        "\n",
        "        # If LR doesn't exist, create it by downsampling HR\n",
        "        if not os.path.exists(lr_path):\n",
        "            lr_image = hr_image.resize((hr_image.width//4, hr_image.height//4), Image.BICUBIC)\n",
        "        else:\n",
        "            lr_image = Image.open(lr_path).convert('RGB')\n",
        "\n",
        "        # Random crop for training\n",
        "        if self.crop_size:\n",
        "            # Get random crop coordinates\n",
        "            hr_width, hr_height = hr_image.size\n",
        "            lr_width, lr_height = lr_image.size\n",
        "\n",
        "            if hr_width >= self.crop_size and hr_height >= self.crop_size:\n",
        "                left = random.randint(0, hr_width - self.crop_size)\n",
        "                top = random.randint(0, hr_height - self.crop_size)\n",
        "\n",
        "                hr_image = hr_image.crop((left, top, left + self.crop_size, top + self.crop_size))\n",
        "\n",
        "                # Corresponding LR crop\n",
        "                lr_left = left // 4\n",
        "                lr_top = top // 4\n",
        "                lr_crop_size = self.crop_size // 4\n",
        "                lr_image = lr_image.crop((lr_left, lr_top, lr_left + lr_crop_size, lr_top + lr_crop_size))\n",
        "\n",
        "        if self.transform:\n",
        "            hr_image = self.transform(hr_image)\n",
        "            lr_image = self.transform(lr_image)\n",
        "\n",
        "        return lr_image, hr_image\n",
        "\n",
        "print(\"‚úÖ DIV2K Dataset class defined!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD-OL7P3ip2b",
        "outputId": "56751164-94c6-491c-f0ec-901679736bfa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DIV2K Dataset class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "\n",
        "# Dataset paths\n",
        "train_hr_path = f'{project_path}/dataset/train/hr'\n",
        "train_lr_path = f'{project_path}/dataset/train/lr'\n",
        "\n",
        "# Check dataset\n",
        "hr_count = len([f for f in os.listdir(train_hr_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "print(f\"üìä Training images: {hr_count}\")\n",
        "\n",
        "if hr_count == 0:\n",
        "    print(\"‚ùå No training images found!\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset ready for training!\")\n",
        "\n",
        "# Check if model already exists\n",
        "if check_trained_model_exists():\n",
        "    choice = input(\"üîç Trained model found! Do you want to:\\n1. Use existing model (skip training)\\n2. Retrain from scratch\\nEnter choice (1 or 2): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        print(\"‚è≠Ô∏è Skipping training, using existing model...\")\n",
        "        skip_training = True\n",
        "    else:\n",
        "        print(\"üîÑ Starting fresh training...\")\n",
        "        skip_training = False\n",
        "else:\n",
        "    print(\"üÜï No existing model found, starting training...\")\n",
        "    skip_training = False\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(in_channels=3, num_channels=64, num_blocks=23).to(device)\n",
        "discriminator = Discriminator(in_channels=3).to(device)\n",
        "\n",
        "# Loss functions\n",
        "criterion_GAN = nn.BCEWithLogitsLoss()\n",
        "criterion_pixelwise = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_gen = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
        "optimizer_disc = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
        "\n",
        "print(\"‚úÖ Models and optimizers initialized!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUv6TZJTirx4",
        "outputId": "030b9520-54cc-48fc-d701-e60e529c473f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Using device: cuda\n",
            "üìä Training images: 700\n",
            "‚úÖ Dataset ready for training!\n",
            "üÜï No existing model found, starting training...\n",
            "‚úÖ Models and optimizers initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only proceed with training if not skipping\n",
        "if not skip_training and hr_count > 0:\n",
        "    # Training parameters\n",
        "    num_epochs = 50  # Adjust based on your needs\n",
        "    batch_size = 4   # Reduce if you get memory errors\n",
        "    crop_size = 128  # Training crop size\n",
        "\n",
        "    print(f\"üèãÔ∏è Starting training with:\")\n",
        "    print(f\"   Epochs: {num_epochs}\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Crop size: {crop_size}\")\n",
        "\n",
        "    # Data transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = DIV2KDataset(train_hr_path, train_lr_path, transform, crop_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    print(f\"üìö Dataset: {len(dataset)} images, {len(dataloader)} batches per epoch\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        generator.train()\n",
        "        discriminator.train()\n",
        "\n",
        "        epoch_gen_loss = 0\n",
        "        epoch_disc_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(dataloader):\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Train Generator\n",
        "            optimizer_gen.zero_grad()\n",
        "\n",
        "            gen_hr = generator(lr_imgs)\n",
        "\n",
        "            # Adversarial loss\n",
        "            pred_real = discriminator(hr_imgs).detach()\n",
        "            pred_fake = discriminator(gen_hr)\n",
        "\n",
        "            loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True),\n",
        "                                    torch.ones_like(pred_fake))\n",
        "\n",
        "            # Pixel-wise loss\n",
        "            loss_pixel = criterion_pixelwise(gen_hr, hr_imgs)\n",
        "\n",
        "            # Total generator loss\n",
        "            loss_gen = loss_GAN + 100 * loss_pixel\n",
        "\n",
        "            loss_gen.backward()\n",
        "            optimizer_gen.step()\n",
        "\n",
        "            # Train Discriminator\n",
        "            optimizer_disc.zero_grad()\n",
        "\n",
        "            pred_real = discriminator(hr_imgs)\n",
        "            pred_fake = discriminator(gen_hr.detach())\n",
        "\n",
        "            loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True),\n",
        "                                     torch.ones_like(pred_real))\n",
        "            loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True),\n",
        "                                     torch.zeros_like(pred_fake))\n",
        "\n",
        "            loss_disc = (loss_real + loss_fake) / 2\n",
        "\n",
        "            loss_disc.backward()\n",
        "            optimizer_disc.step()\n",
        "\n",
        "            epoch_gen_loss += loss_gen.item()\n",
        "            epoch_disc_loss += loss_disc.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # Print batch progress\n",
        "            if i % 50 == 0:\n",
        "                print(f\"   Batch {i}/{len(dataloader)}, Gen: {loss_gen.item():.4f}, Disc: {loss_disc.item():.4f}\")\n",
        "\n",
        "        # Print epoch progress\n",
        "        avg_gen_loss = epoch_gen_loss / batch_count\n",
        "        avg_disc_loss = epoch_disc_loss / batch_count\n",
        "\n",
        "        print(f\"üî• Epoch [{epoch+1}/{num_epochs}] - Gen Loss: {avg_gen_loss:.4f}, Disc Loss: {avg_disc_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            save_model(generator, discriminator, optimizer_gen, optimizer_disc, epoch, avg_gen_loss)\n",
        "            print(f\"üíæ Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(generator, discriminator, optimizer_gen, optimizer_disc, epoch, avg_gen_loss)\n",
        "    print(\"üéâ Training completed and model saved!\")\n",
        "\n",
        "elif skip_training:\n",
        "    print(\"‚è≠Ô∏è Training skipped. Loading existing model...\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot start training - no dataset found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3YndXxKitsy",
        "outputId": "cf37aa5a-41ba-4d91-e984-569599e0dd60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèãÔ∏è Starting training with:\n",
            "   Epochs: 50\n",
            "   Batch size: 4\n",
            "   Crop size: 128\n",
            "üì∏ Found 700 training images\n",
            "üìö Dataset: 700 images, 175 batches per epoch\n",
            "   Batch 0/175, Gen: 47.4502, Disc: 0.6973\n",
            "   Batch 50/175, Gen: 18.5473, Disc: 0.2548\n",
            "   Batch 100/175, Gen: 16.8367, Disc: 0.2970\n",
            "   Batch 150/175, Gen: 13.3763, Disc: 0.2540\n",
            "üî• Epoch [1/50] - Gen Loss: 17.5519, Disc Loss: 0.3581\n",
            "   Batch 0/175, Gen: 16.0932, Disc: 0.0630\n",
            "   Batch 50/175, Gen: 11.7477, Disc: 0.2544\n",
            "   Batch 100/175, Gen: 18.8874, Disc: 0.1916\n",
            "   Batch 150/175, Gen: 12.9050, Disc: 0.2720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_single_image(image_path, output_path=None):\n",
        "    \"\"\"Perform super-resolution on a single image\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize generator\n",
        "    generator = Generator(in_channels=3, num_channels=64, num_blocks=23).to(device)\n",
        "\n",
        "    # Load trained model\n",
        "    if not load_model_for_inference(generator, device):\n",
        "        return None\n",
        "\n",
        "    # Load and preprocess image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    original_size = img.size\n",
        "\n",
        "    # Ensure dimensions are divisible by 4 for proper upsampling\n",
        "    new_width = (img.width // 4) * 4\n",
        "    new_height = (img.height // 4) * 4\n",
        "    img = img.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate high-resolution image\n",
        "    with torch.no_grad():\n",
        "        hr_img = generator(img_tensor)\n",
        "\n",
        "    # Convert back to PIL Image\n",
        "    hr_img = hr_img.squeeze(0).cpu()\n",
        "    hr_img = (hr_img + 1) / 2  # Denormalize\n",
        "    hr_img = torch.clamp(hr_img, 0, 1)\n",
        "    hr_img = transforms.ToPILImage()(hr_img)\n",
        "\n",
        "    # Save or return result\n",
        "    if output_path:\n",
        "        hr_img.save(output_path)\n",
        "        print(f\"üíæ Super-resolution image saved to {output_path}\")\n",
        "\n",
        "    return hr_img, img\n",
        "\n",
        "def display_comparison(lr_image, hr_image):\n",
        "    \"\"\"Display comparison between low-res and super-res images\"\"\"\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Low resolution image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(lr_image)\n",
        "    plt.title(f'Input Image\\nSize: {lr_image.size}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Super resolution image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(hr_image)\n",
        "    plt.title(f'Super-Resolution (4x)\\nSize: {hr_image.size}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Inference functions defined!\")\n"
      ],
      "metadata": {
        "id": "EZJ3OYjFiwcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained model\n",
        "print(\"üß™ Testing the trained model...\")\n",
        "\n",
        "# Load a sample image from validation set for testing\n",
        "valid_hr_path = f'{project_path}/dataset/valid/hr'\n",
        "valid_images = [f for f in os.listdir(valid_hr_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "if valid_images and check_trained_model_exists():\n",
        "    # Test with first validation image\n",
        "    test_image_path = os.path.join(valid_hr_path, valid_images[0])\n",
        "    output_path = f\"{project_path}/results/test_super_res.png\"\n",
        "\n",
        "    print(f\"üîç Testing with: {valid_images[0]}\")\n",
        "\n",
        "    # Create a low-res version for testing\n",
        "    test_hr = Image.open(test_image_path).convert('RGB')\n",
        "    test_lr = test_hr.resize((test_hr.width//4, test_hr.height//4), Image.BICUBIC)\n",
        "    test_lr_path = f\"{project_path}/results/test_input.png\"\n",
        "    test_lr.save(test_lr_path)\n",
        "\n",
        "    # Perform super-resolution\n",
        "    result_hr, input_lr = inference_single_image(test_lr_path, output_path)\n",
        "\n",
        "    if result_hr:\n",
        "        print(\"‚úÖ Super-resolution successful!\")\n",
        "        display_comparison(input_lr, result_hr)\n",
        "\n",
        "        # Also show the original HR for comparison\n",
        "        plt.figure(figsize=(20, 7))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(input_lr)\n",
        "        plt.title(f'Input LR\\n{input_lr.size}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(result_hr)\n",
        "        plt.title(f'Generated SR\\n{result_hr.size}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(test_hr)\n",
        "        plt.title(f'Original HR\\n{test_hr.size}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.suptitle('Comparison: Input LR ‚Üí Generated SR ‚Üí Original HR')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"‚ùå Super-resolution failed!\")\n",
        "\n",
        "elif not check_trained_model_exists():\n",
        "    print(\"‚ùå No trained model found! Please train the model first.\")\n",
        "else:\n",
        "    print(\"‚ùå No validation images found for testing!\")\n"
      ],
      "metadata": {
        "id": "AJlmfRcGizZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def upload_and_test():\n",
        "    \"\"\"Upload your own image and test super-resolution\"\"\"\n",
        "    print(\"üì§ Upload an image to test super-resolution:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"üîç Processing: {filename}\")\n",
        "\n",
        "        # Perform super-resolution\n",
        "        output_filename = f\"super_res_{filename}\"\n",
        "        output_path = f\"{project_path}/results/{output_filename}\"\n",
        "\n",
        "        result_hr, input_img = inference_single_image(filename, output_path)\n",
        "\n",
        "        if result_hr:\n",
        "            print(\"‚úÖ Success!\")\n",
        "            display_comparison(input_img, result_hr)\n",
        "\n",
        "            # Download the result\n",
        "            files.download(output_path)\n",
        "        else:\n",
        "            print(\"‚ùå Failed to process image!\")\n",
        "\n",
        "# Uncomment to test with your own image:\n",
        "# upload_and_test()\n",
        "\n",
        "print(\"‚úÖ Upload and test function ready!\")\n",
        "print(\"üìù Run upload_and_test() to test with your own images\")\n"
      ],
      "metadata": {
        "id": "Mw1OAb4CizUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0uJU7lZfizOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}